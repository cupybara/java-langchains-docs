"use strict";(self.webpackChunkjava_langchains_docs=self.webpackChunkjava_langchains_docs||[]).push([[275],{3905:(t,e,n)=>{n.d(e,{Zo:()=>u,kt:()=>h});var o=n(7294);function s(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function r(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(t);e&&(o=o.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,o)}return n}function a(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?r(Object(n),!0).forEach((function(e){s(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function i(t,e){if(null==t)return{};var n,o,s=function(t,e){if(null==t)return{};var n,o,s={},r=Object.keys(t);for(o=0;o<r.length;o++)n=r[o],e.indexOf(n)>=0||(s[n]=t[n]);return s}(t,e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);for(o=0;o<r.length;o++)n=r[o],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(s[n]=t[n])}return s}var c=o.createContext({}),m=function(t){var e=o.useContext(c),n=e;return t&&(n="function"==typeof t?t(e):a(a({},e),t)),n},u=function(t){var e=m(t.components);return o.createElement(c.Provider,{value:e},t.children)},p="mdxType",l={inlineCode:"code",wrapper:function(t){var e=t.children;return o.createElement(o.Fragment,{},e)}},d=o.forwardRef((function(t,e){var n=t.components,s=t.mdxType,r=t.originalType,c=t.parentName,u=i(t,["components","mdxType","originalType","parentName"]),p=m(n),d=s,h=p["".concat(c,".").concat(d)]||p[d]||l[d]||r;return n?o.createElement(h,a(a({ref:e},u),{},{components:n})):o.createElement(h,a({ref:e},u))}));function h(t,e){var n=arguments,s=e&&e.mdxType;if("string"==typeof t||s){var r=n.length,a=new Array(r);a[0]=d;var i={};for(var c in e)hasOwnProperty.call(e,c)&&(i[c]=e[c]);i.originalType=t,i[p]="string"==typeof t?t:s,a[1]=i;for(var m=2;m<r;m++)a[m]=n[m];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},3664:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>l,frontMatter:()=>r,metadata:()=>i,toc:()=>m});var o=n(7462),s=(n(7294),n(3905));const r={sidebar_position:4},a="QA",i={unversionedId:"packages/qa",id:"packages/qa",title:"QA",description:"Modify Documents",source:"@site/docs/packages/qa.md",sourceDirName:"packages",slug:"/packages/qa",permalink:"/docs/packages/qa",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/packages/qa.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"LLM",permalink:"/docs/packages/llm"},next:{title:"Use Cases",permalink:"/docs/category/use-cases"}},c={},m=[{value:"Modify Documents",id:"modify-documents",level:2},{value:"Combine Documents",id:"combine-documents",level:2},{value:"Map LLM results to answers with sources",id:"map-llm-results-to-answers-with-sources",level:2},{value:"Split Documents",id:"split-documents",level:2}],u={toc:m},p="wrapper";function l(t){let{components:e,...n}=t;return(0,s.kt)(p,(0,o.Z)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"qa"},"QA"),(0,s.kt)("h2",{id:"modify-documents"},"Modify Documents"),(0,s.kt)("p",null,"The ModifyDocumentsContentChain can be used for document summarization (for example)."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'// create the llm chain which is used for summarization\nLargeLanguageModelChain llmChain = new OpenAiChatCompletionsChain(\n        PromptTemplates.QA_SUMMARIZE, \n        new OpenAiChatCompletionsParameters().temperature(0D).model("gpt-3.5-turbo"),\n        System.getenv("OPENAI_API_KEY"));\n\n// create the ModifyDocumentsContentChain which is used to apply the llm chain to each passed document\nModifyDocumentsContentChain summarizeDocumentsChain = new ModifyDocumentsContentChain(llmChain);\n\n// create some example documents\nMap<String, String> myFirstDocument = new HashMap<String, String>();\nmyFirstDocument.put(PromptConstants.CONTENT, "this is my first document content");\nmyFirstDocument.put(PromptConstants.SOURCE, "this is my first document source");\n// the default summarize prompt PromptTemplates.QA_SUMMARIZE also expects the question used for retrieval in the document\nmyFirstDocument.put(PromptConstants.QUESTION, "who is John Doe?");\n\nMap<String, String> mySecondDocument = new HashMap<String, String>();\nmySecondDocument.put(PromptConstants.CONTENT, "this is my second document content");\nmySecondDocument.put(PromptConstants.SOURCE, "this is my second document source");\nmySecondDocument.put(PromptConstants.QUESTION, "how old is John Doe?"); // see comment above\n\n// input for the summarize chain is a stream of documents\nStream<Map<String, String>> documents = Stream.of(myFirstDocument, mySecondDocument);\n\n// output contains the passed documents with summarized content-Value\nStream<Map<String, String>> summarizedDocuments = summarizeDocumentsChain.run(documents);\n')),(0,s.kt)("h2",{id:"combine-documents"},"Combine Documents"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'CombineDocumentsChain combineDocumentsChain = new CombineDocumentsChain();\n\nMap<String, String> myFirstDocument = new HashMap<String, String>();\nmyFirstDocument.put(PromptConstants.CONTENT, "this is my first document content");\nmyFirstDocument.put(PromptConstants.SOURCE, "this is my first document source");\n\nMap<String, String> mySecondDocument = new HashMap<String, String>();\nmySecondDocument.put(PromptConstants.CONTENT, "this is my second document content");\nmySecondDocument.put(PromptConstants.SOURCE, "this is my second document source");\n\nStream<Map<String, String>> documents = Stream.of(myFirstDocument, mySecondDocument);\n\nMap<String, String> combinedDocument = combineDocumentsChain.run(documents);\n/* \n * Content: this is my first document content\n * Source: this is my first document source\n *\n * Content: this is my second document content\n * Source: this is my second document source\n * \n * (stored with key "content" inside the map)\n */\n')),(0,s.kt)("h2",{id:"map-llm-results-to-answers-with-sources"},"Map LLM results to answers with sources"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'MapAnswerWithSourcesChain mapAnswerWithSourcesChain = new MapAnswerWithSourcesChain();\n\nAnswerWithSources answerWithSources = mapAnswerWithSourcesChain.run("The answer is bla bla bla.\\nSOURCES: page 1 book xy, page 2 book ab");\n\nSystem.out.println(answerWithSources.getAnswer());  // The answer is bla bla bla.\nSystem.out.println(answerWithSources.getSources()); // [page 1 book xy, page 2 book ab]\n\n')),(0,s.kt)("h2",{id:"split-documents"},"Split Documents"),(0,s.kt)("p",null,"See ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/cupybara/java-langchains/tree/master/src/test/java/io/github/cupybara/javalangchains/chains/qa/split/SplitDocumentsChainTest.java"},"SplitDocumentsChainTest")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-java"},'\n// 1. Create Documents\n\nList<Map<String, String>> documents = new LinkedList<>();\n\nMap<String, String> firstDocument = new LinkedHashMap<>();\nfirstDocument.put(PromptConstants.SOURCE, "book of john");\nfirstDocument.put(PromptConstants.CONTENT, "This is a short text. This is another short text.");\ndocuments.add(firstDocument);\n\nMap<String, String> secondDocument = new LinkedHashMap<>();\nsecondDocument.put(PromptConstants.SOURCE, "book of jane");\nsecondDocument.put(PromptConstants.CONTENT, "This is a short text.");\ndocuments.add(secondDocument);\n\n// 2. Split Documents\n\n/*\n * We create a TextSplitter that splits a text into partitions using a JTokkit\n * Encoding. We use the cl100k_base encoding (which btw is the default for\n * gpt-3.5-turbo)\n */\nTextSplitter textSplitter = new JtokkitTextSplitter(\n        Encodings.newDefaultEncodingRegistry().getEncoding(EncodingType.CL100K_BASE), 10);\n\n/*\n * we now instantiate the SplitDocumentsChain which will split our documents\n * using the above created TextSplitter on the "content" field.\n */\nSplitDocumentsChain splitDocumentsChain = new SplitDocumentsChain(textSplitter);\n\nList<Map<String, String>> splitDocuments = splitDocumentsChain.run(documents.stream())\n        .collect(Collectors.toList());\n\n// splitDocuments: [\n//   {content=This is a short text. , source=book of john},\n//   {content=This is another short text., source=book of john},\n//   {content=This is a short text., source=book of jane}\n// ]\n')))}l.isMDXComponent=!0}}]);